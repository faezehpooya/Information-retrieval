{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class Searcher:\n",
    "    def __init__(self, ind):\n",
    "        self.indexer = ind\n",
    "        self.K = 10\n",
    "        self.tf_ratio = 0.5\n",
    "\n",
    "    def query_weight(self, query: list):\n",
    "        term_frequency = {}\n",
    "        \n",
    "        for term in query:\n",
    "            if term not in term_frequency.keys():\n",
    "                term_frequency[term] = 1\n",
    "            else:\n",
    "                term_frequency[term] += 1\n",
    "                \n",
    "        query_vector = {}\n",
    "#       l\n",
    "        for term in term_frequency.keys():\n",
    "            query_vector[term] = 1 + np.log(term_frequency[term])\n",
    "#       t\n",
    "#         for term in term_frequency.keys():\n",
    "#             N = len(self.indexer.all_docs)\n",
    "#             query_vector[term] *= (np.log(N) - np.log(self.indexer.get_df(term)))\n",
    "#       c\n",
    "        summation = 0\n",
    "        for term in term_frequency.keys():\n",
    "            summation += query_vector[term] * query_vector[term] \n",
    "        summation = math.sqrt(summation)\n",
    "        \n",
    "        for term in term_frequency.keys():\n",
    "            query_vector[term] /= summation\n",
    "            \n",
    "        return query_vector\n",
    "\n",
    "    def doc_weight_parametric(self, doc_ids: list, query_vector: dict, parameter):\n",
    "        doc_weight_dict = {}  # {doc_id: weight}\n",
    "        N = len(self.indexer.all_docs)\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            doc_vector = {}\n",
    "            for term in query_vector.keys():\n",
    "                # return value of get_tf is a dict: {'title': tf in title, 'description': tf in desvription}\n",
    "                tf_dic = self.indexer.get_tf(term, doc_id)\n",
    "                tf = tf_dic[parameter]\n",
    "                l_term = 0\n",
    "                if tf > 0:\n",
    "                    l_term = 1 + np.log(tf)\n",
    "                l_term *= (np.log(N) - np.log(self.indexer.get_df(term)))\n",
    "                doc_vector[term] = l_term\n",
    "            summation = 0\n",
    "            for l in doc_vector.values():\n",
    "                summation += l * l\n",
    "            summation = math.sqrt(summation)\n",
    "            if summation != 0:\n",
    "                for v in doc_vector.keys():\n",
    "                    doc_vector[v] /= summation\n",
    "\n",
    "            doc_weight_dict[doc_id] = doc_vector\n",
    "        \n",
    "        return doc_weight_dict  # {doc_id: {term_id: weight}}\n",
    "\n",
    "    def doc_weight(self, doc_ids: list, query_vector: dict, parameter=None):\n",
    "\n",
    "        doc_weight_dict_pre = {}\n",
    "        doc_weight_dict = {}  # {doc_id: weight}\n",
    "\n",
    "        \n",
    "        if parameter:\n",
    "            doc_weight_dict_param = self.doc_weight_parametric(doc_ids, query_vector,\n",
    "                                                             parameter)  # {doc_id: {term_id: weight}}\n",
    "            for doc_id in doc_weight_dict_param.keys():\n",
    "                weight = 0\n",
    "                for term_id in query_vector.keys():\n",
    "                    weight += query_vector[term_id] * doc_weight_dict_param[doc_id][term_id]\n",
    "                doc_weight_dict[doc_id] = weight\n",
    "        else:\n",
    "            doc_weight_dict_title = self.doc_weight_parametric(doc_ids, query_vector,\n",
    "                                                             'title')  # {doc_id: {term_id: weight}}\n",
    "            doc_weight_dict_des = self.doc_weight_parametric(doc_ids, query_vector,\n",
    "                                                             'description')  # {doc_id: {term_id: weight}}\n",
    "            for doc_id in doc_weight_dict_title.keys():\n",
    "                weight = 0\n",
    "                for term_id in query_vector.keys():\n",
    "                    weight += query_vector[term_id] * doc_weight_dict_title[doc_id][term_id]\n",
    "                doc_weight_dict[doc_id] = self.tf_ratio * weight\n",
    "            for doc_id in doc_weight_dict_des.keys():\n",
    "                weight = 0\n",
    "                for term_id in query_vector.keys():\n",
    "                    weight += query_vector[term_id] * doc_weight_dict_des[doc_id][term_id]\n",
    "                doc_weight_dict[doc_id] += (1 - self.tf_ratio) * weight\n",
    "            \n",
    "\n",
    "        # doc_weight_dict : {doc_id: weight}\n",
    "\n",
    "        top_docs = sorted(doc_weight_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        if len(top_docs) > self.K:\n",
    "            top_docs = top_docs[0: self.K]\n",
    "        top_doc_ids = [i[0] for i in top_docs]\n",
    "        return top_doc_ids\n",
    "\n",
    "    def search(self, query: str, parameter=None):\n",
    "        query = query.split()\n",
    "        positional_index = self.indexer.positional_index\n",
    "\n",
    "        query_vector = self.query_weight(query)\n",
    "        term_ids = []\n",
    "        for term in query_vector.keys():\n",
    "            if self.indexer.dict_terms.get(term):\n",
    "                term_ids.append(self.indexer.dict_terms[term]['t_id'])\n",
    "\n",
    "        related_doc_ids_list = set()\n",
    "        for term_id in term_ids:\n",
    "            if term_id in positional_index.keys():\n",
    "                docs_related = positional_index[term_id]\n",
    "\n",
    "                if not parameter:\n",
    "                    for doc_id in docs_related.keys():\n",
    "                        related_doc_ids_list = set.union(\n",
    "                            related_doc_ids_list,\n",
    "                            set([doc_id])\n",
    "                        )\n",
    "                else:\n",
    "                    for doc_id, v in docs_related.items():\n",
    "                        if v.get(parameter) and not len(v[parameter]) == 0:\n",
    "                            related_doc_ids_list = set.union(\n",
    "                                related_doc_ids_list,\n",
    "                                set([doc_id])\n",
    "                            )  # todo: intersection?\n",
    "\n",
    "        related_doc_ids_list = list(related_doc_ids_list)\n",
    "        return self.doc_weight(related_doc_ids_list, query_vector, parameter)\n",
    "\n",
    "    \n",
    "    def proximity_search(self, query: str, window_size: int, parameter=None):\n",
    "        query = query.split()\n",
    "        if len(query) <= 0:\n",
    "            return []\n",
    "        positional_index = self.indexer.positional_index\n",
    "        related_doc_ids_list = set(self.indexer.all_docs)\n",
    "\n",
    "#         term = query[0]\n",
    "#         term_id = self.indexer.dict_terms[term]['t_id']\n",
    "#         if term_id in positional_index:\n",
    "#             docs_related = positional_index[term_id]\n",
    "#             related_doc_ids_list = set(list(docs_related.keys()))\n",
    "\n",
    "        for term in query:\n",
    "            term_id = self.indexer.dict_terms[term]['t_id']\n",
    "            if term_id in positional_index:\n",
    "                docs_related = positional_index[term_id]\n",
    "\n",
    "                related_doc_ids_list = set.intersection(\n",
    "                    related_doc_ids_list,\n",
    "                    set(list(docs_related.keys()))\n",
    "                )\n",
    "\n",
    "        query_vector = self.query_weight(query)\n",
    "        related_doc_ids_list = list(related_doc_ids_list)\n",
    "\n",
    "        term_ids = []\n",
    "        for term in query_vector.keys():\n",
    "            term_ids.append(self.indexer.dict_terms[term]['t_id'])\n",
    "\n",
    "        if parameter:\n",
    "            proximity_docs = self.proximity_parametric(term_ids, parameter, related_doc_ids_list, window_size)\n",
    "        else:\n",
    "            proximity_docs_des = self.proximity_parametric(term_ids, 'description', related_doc_ids_list, window_size)\n",
    "            proximity_docs_title = self.proximity_parametric(term_ids, 'title', related_doc_ids_list, window_size)\n",
    "            proximity_docs = list(set.union(set(proximity_docs_title), set(proximity_docs_des)))\n",
    "\n",
    "        return self.doc_weight(proximity_docs, query_vector)\n",
    "    \n",
    "    def proximity_parametric(self, term_ids, parameter, related_doc_ids_list, window_size):\n",
    "        proximity_docs = []\n",
    "        for doc_id in related_doc_ids_list:\n",
    "            all_postings = {}\n",
    "            heap = []\n",
    "\n",
    "            rel = True\n",
    "            for term_id in term_ids:\n",
    "                title_and_des = self.indexer.positional_index[term_id][doc_id]\n",
    "                if title_and_des.get(parameter):\n",
    "                    all_postings[term_id] = title_and_des[parameter]\n",
    "                else:\n",
    "                    rel = False\n",
    "                    break\n",
    "            if not rel:\n",
    "                continue\n",
    "\n",
    "            for term_id, posting_list in all_postings.items():\n",
    "                index_in_posting = 0\n",
    "                heapq.heappush(heap, (posting_list[index_in_posting], term_id, index_in_posting))\n",
    "\n",
    "\n",
    "            while True:\n",
    "                if (heapq.nlargest(1, heap)[0][0] - heapq.nsmallest(1, heap)[0][0]) < window_size:\n",
    "                    proximity_docs.append(doc_id)\n",
    "                    break\n",
    "                (p, term_id, index_in_posting) = heapq.heappop(heap)\n",
    "                posting_list = all_postings[term_id]\n",
    "                if index_in_posting == len(posting_list) - 1:\n",
    "                    break\n",
    "                new_item = (posting_list[index_in_posting + 1], term_id, index_in_posting + 1)\n",
    "                heapq.heappush(heap, new_item)\n",
    "\n",
    "        return proximity_docs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
