{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from abc import abstractmethod\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import hazm\n",
    "from hazm import Stemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "class read_file:\n",
    "    def __init__(self, csv_file, persion_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.persion_file = persion_file\n",
    "        \n",
    "    def get_doc(self, doc_id, language='English'):\n",
    "        if language == 'persian':\n",
    "            tree = ET.parse(self.persion_file)\n",
    "            root = list(tree.getroot())\n",
    "            prefix_element_name = \"{http://www.mediawiki.org/xml/export-0.10/}\"\n",
    "\n",
    "            page = root[doc_id]\n",
    "            title = page.find(prefix_element_name + 'title').text\n",
    "            title = re.sub(' +', ' ', title.strip())\n",
    "            text = page.find(prefix_element_name + 'revision').find(prefix_element_name + 'text').text\n",
    "            text = re.sub(' +', ' ', text.strip())\n",
    "            doc = dict()\n",
    "            doc[\"title\"] = title\n",
    "            doc[\"description\"] = text\n",
    "            \n",
    "        else:\n",
    "            file_path = self.csv_file\n",
    "            with open(file_path, mode='r', encoding=\"utf8\") as csv_file:\n",
    "                csv_reader = list(csv.DictReader(csv_file))\n",
    "                row = csv_reader[doc_id]\n",
    "                title = re.sub(' +', ' ', row[\"title\"].strip())\n",
    "                description = re.sub(' +', ' ', row[\"description\"].strip())\n",
    "                doc = dict()\n",
    "                doc[\"title\"] = title\n",
    "                doc[\"description\"] = description\n",
    "                \n",
    "        return doc\n",
    "            \n",
    "        \n",
    "\n",
    "    def read_csv_file_as_list(self):\n",
    "        file_path = self.csv_file\n",
    "        eng_list = []\n",
    "        with open(file_path, mode='r', encoding=\"utf8\") as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            for row in csv_reader:\n",
    "                title = re.sub(' +', ' ', row[\"title\"].strip())\n",
    "                description = re.sub(' +', ' ', row[\"description\"].strip())\n",
    "                d = dict()\n",
    "                d[\"title\"] = title\n",
    "                d[\"description\"] = description\n",
    "                eng_list.append(d)\n",
    "            return eng_list\n",
    "\n",
    "    def read_persian_xml_file_as_list(self):\n",
    "        tree = ET.parse(self.persion_file)\n",
    "        root = tree.getroot()\n",
    "        prefix_element_name = \"{http://www.mediawiki.org/xml/export-0.10/}\"\n",
    "        per_list = []\n",
    "        for page in root:\n",
    "            title = page.find(prefix_element_name + 'title').text\n",
    "            title = re.sub(' +', ' ', title.strip())\n",
    "            text = page.find(prefix_element_name + 'revision').find(prefix_element_name + 'text').text\n",
    "            text = re.sub(' +', ' ', text.strip())\n",
    "            d = dict()\n",
    "            d[\"title\"] = title\n",
    "            d[\"description\"] = text\n",
    "            per_list.append(d)\n",
    "        return per_list\n",
    "\n",
    "\n",
    "class GenericPreprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processed_list = None\n",
    "        self.high_accur_param = None\n",
    "        self.stop_words = None\n",
    "        self.high_accured_words = None\n",
    "\n",
    "    def preprocess(self, text_list, is_query=False):\n",
    "        \"\"\"\n",
    "        :param text_list: ['text']\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.processed_list = []\n",
    "        normalized_list = []\n",
    "        if not is_query:\n",
    "            for news in text_list:\n",
    "                self.processed_list.append(\n",
    "                    {\"title\": self.normalize(news[\"title\"]), \"description\": self.normalize(news[\"description\"])})\n",
    "\n",
    "            self.set_stopwords()\n",
    "            self.remove_stopwords()\n",
    "\n",
    "            for news in self.processed_list:\n",
    "                title = news[\"title\"]\n",
    "                description = news[\"description\"]\n",
    "                title_stem = self.__stem_doc(title)\n",
    "                description_stem = self.__stem_doc(description)\n",
    "                news[\"title\"] = title_stem\n",
    "                news[\"description\"] = description_stem\n",
    "                normalized_list.append(news)\n",
    "            self.processed_list = normalized_list\n",
    "\n",
    "        else:\n",
    "            processed_list = []\n",
    "            for news in text_list:\n",
    "                processed_list.append(self.normalize(news))\n",
    "\n",
    "#             self.set_stopwords()\n",
    "#             self.remove_stopwords()\n",
    "\n",
    "            for news in processed_list:\n",
    "                text = self.__stem_doc(news)\n",
    "                if self.stop_words:\n",
    "                    for word in text.split():\n",
    "                        if word not in self.stop_words:\n",
    "                            normalized_list.append(word)\n",
    "                else:\n",
    "                    normalized_list.append(text)\n",
    "            processed_list = normalized_list\n",
    "            normalized_list = ' '.join(normalized_list)\n",
    "\n",
    "\n",
    "        return normalized_list\n",
    "\n",
    "    def __stem_doc(self, doc):\n",
    "        normalized_words = []\n",
    "        for word in self.__get_word_by_word(doc):\n",
    "            nword = self.stem(word)\n",
    "            if nword is not None and nword != '':\n",
    "                normalized_words.append(nword)\n",
    "        return ' '.join(normalized_words)\n",
    "\n",
    "    def __get_word_by_word(self, doc_str):\n",
    "        words = self.tokenize(doc_str)\n",
    "        for word in words:\n",
    "            yield word\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize(self, doc_str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def normalize(self, text):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def stem(self, word):\n",
    "        pass\n",
    "\n",
    "    def set_stopwords(self):\n",
    "        self.high_accured_words = self.__find_high_accured_words()\n",
    "        self.stop_words = set()\n",
    "        for key, value in self.high_accured_words.items():\n",
    "            self.stop_words.add(key)\n",
    "\n",
    "    def remove_punctuation(self, word):\n",
    "        return re.sub(r'[^\\w\\s]', '', word)\n",
    "\n",
    "    def __get_accurance_dict(self):\n",
    "        accurance_dict = {}\n",
    "        for news in self.processed_list:\n",
    "            title = news[\"title\"]\n",
    "            description = news[\"description\"]\n",
    "            titlewords = title.split()\n",
    "            descriptionwords = description.split()\n",
    "            words = titlewords + descriptionwords\n",
    "            for word in words:\n",
    "                accurance_dict[word] = accurance_dict.get(word, 0) + 1\n",
    "        return accurance_dict\n",
    "\n",
    "    def __find_high_accured_words(self):\n",
    "        accurance_dict = self.__get_accurance_dict()\n",
    "        accurance_dict = dict(reversed((sorted(accurance_dict.items(), key=lambda x: x[1]))))\n",
    "        high_accured_words = dict()\n",
    "        for key, value in accurance_dict.items():\n",
    "            if value >= self.high_accur_param:\n",
    "                high_accured_words[key] = value\n",
    "        return high_accured_words\n",
    "\n",
    "    def get_high_accured_words(self):\n",
    "        return dict(reversed((sorted(self.high_accured_words.items(), key=lambda x: x[1]))))\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        updated_processed_list = []\n",
    "        for news in self.processed_list:\n",
    "            updated_news = \"\"\n",
    "            title = news[\"title\"]\n",
    "            description = news[\"description\"]\n",
    "            titlewords = title.split()\n",
    "            descriptionwords = description.split()\n",
    "            for word in titlewords:\n",
    "                if word not in self.stop_words:\n",
    "                    updated_news += word + \" \"\n",
    "            news[\"title\"] = updated_news\n",
    "            updated_news = \"\"\n",
    "            for word in descriptionwords:\n",
    "                if word not in self.stop_words:\n",
    "                    updated_news += word + \" \"\n",
    "            news[\"description\"] = updated_news\n",
    "            updated_processed_list.append(news)\n",
    "        self.processed_list = updated_processed_list\n",
    "        return self.processed_list\n",
    "\n",
    "\n",
    "class EnglishPreprocessor(GenericPreprocessor):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.high_accur_param = 1300\n",
    "\n",
    "    def tokenize(self, doc_str):\n",
    "        return nltk.word_tokenize(doc_str)\n",
    "\n",
    "    def normalize(self, text):\n",
    "        text = self.remove_non_ascii(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.lower(text)\n",
    "        text = re.sub(' +', ' ', text.strip())\n",
    "        return text\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.stemmer.stem(word)\n",
    "        word = re.sub(' +', ' ', word.strip())\n",
    "        return word\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_non_ascii(word):\n",
    "        return unicodedata.normalize('NFKD', word) \\\n",
    "            .encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    @staticmethod\n",
    "    def lower(word):\n",
    "        return word.lower()\n",
    "\n",
    "\n",
    "class PersianPreprocessor(GenericPreprocessor):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stemmer = Stemmer()\n",
    "        self.high_accur_param = 3500\n",
    "\n",
    "    def tokenize(self, doc_str):\n",
    "        return hazm.word_tokenize(doc_str)\n",
    "\n",
    "    def normalize(self, text):\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = re.sub(' +', ' ', text.strip())\n",
    "        return text\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.stemmer.stem(word)\n",
    "        word = re.sub(' +', ' ', word.strip())\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
