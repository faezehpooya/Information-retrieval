{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "%run Index/Comperss/VariableByte.ipynb\n",
    "%run Index/Comperss/GammaCode.ipynb\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self, docs_directory='all_docs/', index_file='index.pkl'):\n",
    "        \n",
    "        self.all_docs = []            # list of doc_ids\n",
    "        self.dict_terms = {}          # {term: {'t_id', 'df'}}\n",
    "        self.inv_term_mapping = {}    # {t_id: term}\n",
    "        self.positional_index = {}    # positional indexing {t_id: {doc_id: {'title': [pos], 'description': [pos]}}}\n",
    "        self.bigram_index = {}        # bigram indexing {bigram: {t_id}}\n",
    "        \n",
    "        self.generate_id = 0\n",
    "\n",
    "        self.docs_directory = docs_directory\n",
    "        self.index_file = index_file\n",
    "        \n",
    "        \n",
    "    def save_index(self, file_name=\"index.pkl\"):\n",
    "        pickle.dump(self, open(self.index_file, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(index_file='index.pkl'):\n",
    "        return pickle.load(open(index_file, 'rb'))\n",
    "\n",
    "\n",
    "    def save_posting(self, compression_type=\"NotCompressed\", file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = compression_type + \"_index.txt\"\n",
    "        if compression_type == \"NotCompressed\":\n",
    "            with open(file_name, 'w+') as file:\n",
    "                file.write(str(self.positional_index))\n",
    "                file.close()\n",
    "        elif compression_type == 'GammaCode':\n",
    "            GammaCodeCompressor.write_compress_to_file(self.positional_index, file_name)\n",
    "        elif compression_type == 'VariableByte':\n",
    "            VariableByteCompressor.write_compress_to_file(self.positional_index, file_name)\n",
    "\n",
    "    def load_posting(self, compression_type=\"NotCompressed\", file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = compression_type + \"_index.txt\"\n",
    "        if compression_type == \"NotCompressed\":\n",
    "            with open(file_name, 'r') as file:\n",
    "                self.positional_index = eval(file.read())\n",
    "                file.close()\n",
    "        elif compression_type == 'GammaCode':\n",
    "            self.positional_index = GammaCodeDecompressor.decompress_from_file(file_name)\n",
    "        elif compression_type == 'VariableByte':\n",
    "            self.positional_index = VariableByteDecompressor.decompress_from_file(file_name)\n",
    "            \n",
    "    def save_bigram_posting(self, file_name=\"Bigram_index.txt\"):\n",
    "        with open(file_name, 'w+') as file:\n",
    "            file.write(str(str(self.bigram_index).encode('utf8')))\n",
    "            file.close()\n",
    "\n",
    "    def load_bigram_posting(self, file_name=\"Bigram_index.txt\"):\n",
    "        with open(file_name, 'r') as file:\n",
    "            self.bigram_index = eval(eval(file.read()).decode('utf8'))\n",
    "            file.close()\n",
    "            \n",
    "    def load_doc(self, doc_id):\n",
    "        file = open(self.docs_directory + str(doc_id) + '.txt', 'r')\n",
    "        doc = eval(file.read())\n",
    "        doc['title'] = doc['title'].decode('utf8')\n",
    "        doc['description'] = doc['description'].decode('utf8')\n",
    "        return doc\n",
    "    \n",
    "    def add_doc(self, doc, doc_id):\n",
    "        write_doc = {}\n",
    "        write_doc['title'] = doc['title'].encode('utf8')\n",
    "        write_doc['description'] = doc['description'].encode('utf8')\n",
    "        with open(self.docs_directory + str(doc_id) + '.txt', 'w+') as file:\n",
    "            file.write(str(write_doc))\n",
    "            file.close()\n",
    "        self.all_docs.append(doc_id)\n",
    "\n",
    "        self.add_doc_to_index(doc_id)\n",
    "\n",
    "    def del_doc(self, doc_id):\n",
    "        self.remove_doc_from_index(doc_id)\n",
    "\n",
    "        os.remove(self.doc_directory + str(doc_id) + '.txt')\n",
    "        self.all_docs.remove(doc_id)\n",
    "        \n",
    "    def add_terms(self, doc_id, terms, header):\n",
    "        for i in range(len(terms)):\n",
    "            term = terms[i]\n",
    "            if self.dict_terms.get(term) is None:\n",
    "                term_id = self.generate_id\n",
    "                self.generate_id += 1\n",
    "                self.dict_terms[term] = {'t_id': term_id, 'df': 0}\n",
    "                self.inv_term_mapping[term_id] = term\n",
    "                self.add_term_to_bigram_index(term)\n",
    "            term_id = self.dict_terms[term]['t_id']\n",
    "            \n",
    "            posting = self.positional_index.get(term_id)\n",
    "            if posting is None:\n",
    "                self.positional_index[term_id] = {}\n",
    "                posting = self.positional_index.get(term_id)\n",
    "            \n",
    "            doc_posting = posting.get(doc_id)\n",
    "            if doc_posting is None:\n",
    "                self.dict_terms[term]['df'] += 1\n",
    "        \n",
    "                posting[doc_id] = {}\n",
    "                doc_posting = posting.get(doc_id)\n",
    "\n",
    "            positions = doc_posting.get(header)\n",
    "            if positions is None:\n",
    "                doc_posting[header] = []\n",
    "                positions = doc_posting.get(header)\n",
    "            positions.append(i)\n",
    "        \n",
    "    def add_doc_to_index(self, doc_id):\n",
    "        doc = self.load_doc(doc_id)\n",
    "        \n",
    "        title_terms = doc['title'].split()\n",
    "        self.add_terms(doc_id, title_terms, 'title')\n",
    "        \n",
    "        description_terms = doc['description'].split()\n",
    "        self.add_terms(doc_id, description_terms, 'description')\n",
    "        \n",
    "    def remove_doc_from_index(self, doc_id):\n",
    "        removed_terms = []\n",
    "        for t_id in self.positional_index:\n",
    "            self.positional_index.get(t_id).pop(doc_id, None)\n",
    "            self.dict_terms[self.inv_term_mapping.get(t_id)]['df'] -= 1\n",
    "            if self.positional_index.get(t_id) == {}:\n",
    "                removed_terms.append(t_id)\n",
    "        for t_id in removed_terms:\n",
    "            self.positional_index.pop(t_id, None)\n",
    "            term = self.inv_term_mapping.pop(t_id, None)\n",
    "            self.remove_term_from_bigram_index(term)\n",
    "            self.dict_terms.pop(term, None)\n",
    "        \n",
    "    def add_term_to_bigram_index(self, term):\n",
    "        t_id = self.dict_terms.get(term)['t_id']\n",
    "        term = '$' + term + '$'\n",
    "        for i in range(len(term) - 1):\n",
    "            bigram = term[i:(i + 2)]\n",
    "            bigrams_t_ids = self.bigram_index.get(bigram)\n",
    "            if bigrams_t_ids is None:\n",
    "                self.bigram_index[bigram] = set()\n",
    "                bigrams_t_ids = self.bigram_index.get(bigram)\n",
    "            bigrams_t_ids.add(t_id)\n",
    "\n",
    "    def remove_term_from_bigram_index(self, term):\n",
    "        t_id = self.dict_terms.get(term)['t_id']\n",
    "        term = '$' + term + '$'\n",
    "        for i in range(len(term) - 1):\n",
    "            bigram = term[i:(i + 2)]\n",
    "            bigrams_t_ids = self.bigram_index.get(bigram)\n",
    "            if t_id in bigrams_t_ids:\n",
    "                bigrams_t_ids.remove(t_id)\n",
    "                \n",
    "    def get_posting(self, term):\n",
    "        term_dict = self.dict_terms.get(term)\n",
    "        if term_dict:\n",
    "            t_id = term_dict.get('t_id')\n",
    "            return list(self.positional_index.get(t_id).keys())\n",
    "        else:\n",
    "            return \"\\\"\" + term + \"\\\" does not exist.\" \n",
    "\n",
    "    def get_posting_with_positions(self, term):\n",
    "        term_dict = self.dict_terms.get(term)\n",
    "        if term_dict:\n",
    "            t_id = term_dict.get('t_id')\n",
    "            return self.positional_index.get(t_id)\n",
    "        else:\n",
    "            return \"\\\"\" + term + \"\\\" does not exist.\" \n",
    "\n",
    "    def get_bigram_posting(self, bigram):\n",
    "        bigrams_t_ids = self.bigram_index.get(bigram)\n",
    "        terms = []\n",
    "        if bigrams_t_ids is not None:\n",
    "            for t_id in bigrams_t_ids:\n",
    "                term = self.inv_term_mapping.get(t_id)\n",
    "                if term is not None:\n",
    "                    terms.append(term)\n",
    "        return terms\n",
    "    \n",
    "    def get_tf(self, term, doc_id):\n",
    "        tf = {'title': 0, 'description': 0}\n",
    "        if not self.dict_terms.get(term):\n",
    "            return tf\n",
    "        t_id = self.dict_terms[term]['t_id']\n",
    "        posting_list = self.positional_index.get(t_id)\n",
    "        if not posting_list.get(doc_id):\n",
    "            return tf\n",
    "        doc_posting = posting_list[doc_id]\n",
    "        if doc_posting.get('title'):\n",
    "            tf['title'] = len(doc_posting['title'])\n",
    "        if doc_posting.get('description'):\n",
    "            tf['description'] = len(doc_posting['description'])\n",
    "        return tf\n",
    "    \n",
    "    def get_df(self, term):\n",
    "        if not self.dict_terms.get(term):\n",
    "            return 0\n",
    "        return self.dict_terms[term]['df']\n",
    "    \n",
    "    def get_number_of_docs(self):\n",
    "        return len(self.all_docs)\n",
    "    \n",
    "    def tf_vector(self, terms, doc_id):\n",
    "        vector = {'title': [], 'description': []}\n",
    "        for term in terms:\n",
    "            tf = self.get_tf(term, doc_id)\n",
    "            title_tf = tf['title']\n",
    "            description_tf = tf['description']\n",
    "            vector['title'].append(title_tf)\n",
    "            vector['description'].append(description_tf)\n",
    "        return vector\n",
    "    \n",
    "    def tf_vector_of_all_docs(self, terms):\n",
    "        tf_vectors = {}\n",
    "        for doc_id in self.all_docs:\n",
    "            tf_vectors[doc_id] = self.tf_vector(terms, doc_id)\n",
    "        return tf_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
